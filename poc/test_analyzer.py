#!/usr/bin/env python3
"""
æµ‹è¯• Analyzer Agent

è¿™ä¸ªè„šæœ¬ç”¨äºå•ç‹¬æµ‹è¯• analyzer çš„è¡Œä¸ºï¼Œ
éªŒè¯å®ƒæ˜¯å¦èƒ½æ­£ç¡®åˆ†æ CSV æ•°æ®å¹¶è¿”å› auto_fixedã€escalations å’Œ valid_rowsã€‚
"""

import os
import json
import logging
from dotenv import load_dotenv
from strands import Agent
from strands.models.openai import OpenAIModel
from strands.telemetry import StrandsTelemetry
from src.prompts import ANALYZE_AND_FIX_PROMPT

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.WARNING,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

logging.getLogger("strands").setLevel(logging.WARNING)
logging.getLogger("strands_tools").setLevel(logging.WARNING)


# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# # è®¾ç½®å¯è§‚æµ‹æ€§
# def setup_observability():
#     """Setup observability with OTLP and console exporters."""
#     strands_telemetry = StrandsTelemetry()
#     strands_telemetry.setup_otlp_exporter()
#     strands_telemetry.setup_meter(
#         enable_console_exporter=False,
#         enable_otlp_exporter=True
#     )

# logger.info("ğŸ”§ è®¾ç½®å¯è§‚æµ‹æ€§...")
# setup_observability()
# logger.info("âœ“ å¯è§‚æµ‹æ€§é…ç½®å®Œæˆ")


def create_test_analyzer():
    """åˆ›å»ºæµ‹è¯•ç”¨çš„ analyzer agent"""
    
    logger.info("ğŸ¤– åˆ›å»º Analyzer Agent...")
    
    # è·å–é…ç½®
    model = os.getenv("MODEL_NAME", "gpt-4")
    temperature = float(os.getenv("TEMPERATURE", "0.3"))
    api_key = os.getenv("OPENAI_API_KEY")
    base_url = os.getenv("OPENAI_BASE_URL")
    max_tokens = int(os.getenv("MAX_TOKENS", "4000"))
    
    if not api_key:
        raise ValueError("OPENAI_API_KEY is required")
    
    logger.info(f"æ¨¡å‹: {model}, æ¸©åº¦: {temperature}, max_tokens: {max_tokens}")
    
    # åˆ›å»ºæ¨¡å‹
    model_instance = OpenAIModel(
        client_args={
            "api_key": api_key,
            "base_url": base_url
        },
        model_id=model,
        params={
            "max_tokens": max_tokens,
            "temperature": temperature,
        }
    )
    
    # ä½¿ç”¨å…±äº«çš„ Pydantic æ¨¡å‹
    from src.models import AnalyzerResult
    
    # åˆ›å»º analyzer agentï¼ˆä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºï¼‰
    analyzer = Agent(
        name="analyzer",
        system_prompt=ANALYZE_AND_FIX_PROMPT,
        tools=[],  # æ²¡æœ‰å·¥å…·
        model=model_instance,
        callback_handler=None,
        structured_output_model=AnalyzerResult  # ä½¿ç”¨ç»“æ„åŒ–è¾“å‡º
    )
    
    logger.info("âœ“ Agent åˆ›å»ºæˆåŠŸï¼ˆä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºï¼‰")
    return analyzer


def parse_analyzer_result(result):
    """è§£æ Analyzer ç»“æœä¸º JSONï¼ˆä½¿ç”¨ structured_outputï¼‰"""
    try:
        if not hasattr(result, 'structured_output'):
            raise AttributeError("result å¯¹è±¡æ²¡æœ‰ structured_output å±æ€§")
        
        if not result.structured_output:
            raise ValueError("structured_output ä¸ºç©º")
        
        logger.info("ä½¿ç”¨ structured_output")
        structured = result.structured_output
        
        # è½¬æ¢ä¸ºå­—å…¸
        if hasattr(structured, 'model_dump'):
            return structured.model_dump(by_alias=True)
        elif hasattr(structured, 'dict'):
            return structured.dict(by_alias=True)
        else:
            raise TypeError(f"æ— æ³•å°† structured_output è½¬æ¢ä¸ºå­—å…¸ï¼Œç±»å‹: {type(structured)}")
    
    except Exception as e:
        logger.error(f"è§£æ structured_output å¤±è´¥: {e}")
        logger.error(f"result ç±»å‹: {type(result)}")
        logger.error(f"result å±æ€§: {dir(result)}")
        if hasattr(result, 'structured_output'):
            logger.error(f"structured_output ç±»å‹: {type(result.structured_output)}")
        raise


def test_simple_data():
    """æµ‹è¯•åœºæ™¯1ï¼šç®€å•çš„æµ‹è¯•æ•°æ®"""
    print("\n" + "="*60)
    print("æµ‹è¯•åœºæ™¯1ï¼šç®€å•çš„æµ‹è¯•æ•°æ®")
    print("="*60)
    
    # æ„å»ºæµ‹è¯•æ•°æ®
    csv_data = {
        "success": True,
        "file_path": "test.csv",
        "row_count": 5,
        "rows": [
            {
                "_row_number": 1,
                "name": "å¼ ä¸‰",
                "gender": "ç”·",
                "title": "å·¥ç¨‹å¸ˆ",
                "email": "zhangsan@example.com",
                "mobile": "13812345678",
                "wechat": "zhangsan_wx",
                "remark": ""
            },
            {
                "_row_number": 2,
                "name": "æå››",
                "gender": "å¥³",
                "title": "éƒ¨é—¨ç»ç†",
                "email": "lisi@@example.com",  # éœ€è¦è‡ªåŠ¨ä¿®å¤ï¼šé‡å¤@
                "mobile": "13987654321",
                "wechat": "",
                "remark": ""
            },
            {
                "_row_number": 3,
                "name": "ç‹äº”",
                "gender": "ç”·æ€§",  # éœ€è¦è‡ªåŠ¨ä¿®å¤ï¼šæ ‡å‡†åŒ–ä¸º"ç”·"
                "title": "é«˜çº§å·¥ç¨‹å¸ˆ",
                "email": "wangwu@example.com",
                "mobile": "138-1234-5678",  # éœ€è¦è‡ªåŠ¨ä¿®å¤ï¼šåˆ é™¤æ ¼å¼åŒ–å­—ç¬¦
                "wechat": "wangwu",
                "remark": ""
            },
            {
                "_row_number": 4,
                "name": "èµµå…­",
                "gender": "å¥³",
                "title": "é¡¾é—®",  # éœ€è¦ escalationï¼šä¸åœ¨æœ‰æ•ˆèŒä½åˆ—è¡¨ä¸­
                "email": "zhaoliu@example.com",
                "mobile": "13912345678",
                "wechat": "",
                "remark": ""
            },
            {
                "_row_number": 5,
                "name": "å­™ä¸ƒ",
                "gender": "ç”·",
                "title": "é¡¾é—®",  # éœ€è¦ escalationï¼šä¸åœ¨æœ‰æ•ˆèŒä½åˆ—è¡¨ä¸­
                "email": "sunqi@example.com",
                "mobile": "136416543",  # éœ€è¦ escalationï¼šä½æ•°ä¸è¶³
                "wechat": "",
                "remark": ""
            }
        ]
    }
    
    print("\nğŸ“‹ è¾“å…¥æ•°æ®:")
    print(f"æ€»è¡Œæ•°: {csv_data['row_count']}")
    print("åŒ…å«ä»¥ä¸‹é—®é¢˜:")
    print("  - ç¬¬2è¡Œ: email æœ‰é‡å¤@ (éœ€è¦è‡ªåŠ¨ä¿®å¤)")
    print("  - ç¬¬3è¡Œ: gender ä¸º'ç”·æ€§' (éœ€è¦è‡ªåŠ¨ä¿®å¤)")
    print("  - ç¬¬3è¡Œ: mobile æœ‰æ ¼å¼åŒ–å­—ç¬¦ (éœ€è¦è‡ªåŠ¨ä¿®å¤)")
    print("  - ç¬¬4è¡Œ: title ä¸º'é¡¾é—®' (éœ€è¦ escalation)")
    print("  - ç¬¬5è¡Œ: title ä¸º'é¡¾é—®' + mobile åªæœ‰9ä½ (éœ€è¦ escalationï¼Œä¸€è¡Œå¤šä¸ªé—®é¢˜)")
    
    # æ„å»ºä»»åŠ¡
    csv_json = json.dumps(csv_data, ensure_ascii=False, indent=2)
    task = f"è¯·åˆ†æä»¥ä¸‹CSVæ•°æ®å¹¶è¿›è¡Œæ•°æ®æ¸…ç†ï¼š\n\n{csv_json}"
    
    # åˆ›å»º analyzer
    analyzer = create_test_analyzer()
    
    # æ‰§è¡Œ
    print("\nğŸ¤– Analyzer æ‰§è¡Œä¸­...")
    logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œ Analyzer...")
    try:
        result = analyzer(task)
        logger.info("âœ“ Analyzer æ‰§è¡Œå®Œæˆ")
    except Exception as e:
        logger.error(f"âœ— Analyzer æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        raise
    
    print("\nâœ… Analyzer è¾“å‡º:")
    print(result)
    
    # è§£æç»“æœ
    try:
        parsed_dict = parse_analyzer_result(result)
        logger.info("âœ“ ç»“æœè§£ææˆåŠŸ")
        
        print("\nğŸ“Š è§£æåçš„ç»“æœ:")
        print(json.dumps(parsed_dict, ensure_ascii=False, indent=2))
        
        # éªŒè¯ç»“æœ
        print("\nğŸ” éªŒè¯ç»“æœ:")
        print(f"  æ€»è¡Œæ•°: {parsed_dict.get('total_rows', 'N/A')}")
        print(f"  è‡ªåŠ¨ä¿®å¤æ•°é‡: {len(parsed_dict.get('auto_fixed', []))}")
        print(f"  éœ€è¦ escalation æ•°é‡: {len(parsed_dict.get('escalations', []))}")
        print(f"  å®Œå…¨æ­£å¸¸è¡Œæ•°é‡: {len(parsed_dict.get('valid_rows', []))}")
        
        if parsed_dict.get('auto_fixed'):
            print("\n  è‡ªåŠ¨ä¿®å¤è¯¦æƒ…:")
            for auto_fixed in parsed_dict['auto_fixed']:
                row_num = auto_fixed['_row_number']
                fixes = auto_fixed['fixes']
                if len(fixes) == 1:
                    fix = fixes[0]
                    print(f"    - ç¬¬{row_num}è¡Œ {fix['column']}: {fix['old_value']} â†’ {fix['new_value']}")
                else:
                    print(f"    - ç¬¬{row_num}è¡Œæœ‰{len(fixes)}ä¸ªä¿®å¤:")
                    for i, fix in enumerate(fixes, 1):
                        print(f"      {i}. {fix['column']}: {fix['old_value']} â†’ {fix['new_value']}")
        
        if parsed_dict.get('escalations'):
            print("\n  Escalation è¯¦æƒ…:")
            for esc in parsed_dict['escalations']:
                row_num = esc['_row_number']
                issues = esc['issues']
                if len(issues) == 1:
                    issue = issues[0]
                    print(f"    - ç¬¬{row_num}è¡Œ {issue['column']}: {issue['issue_type']} - {issue['description']}")
                else:
                    print(f"    - ç¬¬{row_num}è¡Œæœ‰{len(issues)}ä¸ªé—®é¢˜:")
                    for i, issue in enumerate(issues, 1):
                        print(f"      {i}. {issue['column']}: {issue['issue_type']} - {issue['description']}")
        
    except Exception as e:
        print(f"\nâš ï¸ ç»“æœè§£æå¤±è´¥: {e}")
        logger.error(f"ç»“æœè§£æå¤±è´¥: {e}", exc_info=True)
        print("åŸå§‹è¾“å‡º:", result)


def main():
    """ä¸»å‡½æ•°"""
    print("\nğŸ§ª Analyzer Agent æµ‹è¯•")
    print("\nè¿™ä¸ªè„šæœ¬æµ‹è¯• analyzer æ˜¯å¦èƒ½ï¼š")
    print("  1. æ­£ç¡®åˆ†æ CSV æ•°æ®")
    print("  2. è¯†åˆ«å¯ä»¥è‡ªåŠ¨ä¿®å¤çš„é—®é¢˜")
    print("  3. è¯†åˆ«éœ€è¦ escalation çš„é—®é¢˜")
    print("  4. è¿”å›æ­£ç¡®æ ¼å¼çš„ JSON")
    
    test_simple_data()
    
    print("\n" + "="*60)
    print("æµ‹è¯•å®Œæˆï¼")
    print("="*60)
    
    print("\nâœ… éªŒè¯è¦ç‚¹ï¼š")
    print("  1. Analyzer æ˜¯å¦å¤„ç†äº†æ‰€æœ‰è¡Œï¼Ÿ")
    print("  2. æ˜¯å¦æ­£ç¡®è¯†åˆ«äº†éœ€è¦è‡ªåŠ¨ä¿®å¤çš„é—®é¢˜ï¼Ÿ")
    print("  3. æ˜¯å¦æ­£ç¡®è¯†åˆ«äº†éœ€è¦ escalation çš„é—®é¢˜ï¼Ÿ")
    print("  4. è¾“å‡ºçš„ JSON æ ¼å¼æ˜¯å¦æ­£ç¡®ï¼Ÿ")
    print("  5. auto_fixed æ˜¯å¦åŒ…å« fixed_rowï¼Ÿ")
    print("  6. escalations æ˜¯å¦åŒ…å« current_rowï¼Ÿ")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\næµ‹è¯•è¢«ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
